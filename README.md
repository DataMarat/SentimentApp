# SentimentApp — развёртывание в Kubernetes

Этот репозиторий содержит Java-приложение для анализа тональности текста и инфраструктурные артефакты для его развёртывания в Kubernetes-кластере на базе Minikube, а также для мониторинга с помощью Prometheus и Grafana.

## Общая идея

Цель работы — показать полный цикл развёртывания микросервиса:

- сборка Docker-образа Java-приложения;
- публикация образа в Docker Hub;
- развёртывание в кластере Kubernetes (Minikube с несколькими нодами);
- настройка балансировки нагрузки и горизонтального автоскейлинга (HPA);
- подключение мониторинга на уровне кластера и приложения (Prometheus + Grafana).

В качестве площадки использована виртуальная машина в Яндекс.Облаке, на которой установлены Docker, kubectl, Minikube и Helm.

## Структура

Основные части репозитория:

- исходный код Java-приложения (Spring Boot, REST-API для анализа тональности);
- Dockerfile для сборки контейнерного образа;
- каталог `k8s/` с манифестами Kubernetes:
    - namespace для приложения;
    - Deployment c несколькими репликами сервиса;
    - Service для публикации HTTP-интерфейса внутри кластера и доступа извне;
    - HorizontalPodAutoscaler (HPA) для масштабирования по CPU;
    - ServiceMonitor и дополнительные настройки для интеграции с Prometheus;
    - values-файлы для вспомогательного Prometheus, собирающего метрики приложения.

Дополнительно в отчёте по инфраструктуре описаны шаги установки Minikube, Docker, Helm и kube-prometheus-stack.

## Развёртывание

Высокоуровневая последовательность действий:

1. На виртуальной машине создаётся Kubernetes-кластер Minikube с драйвером Docker и двумя нодами, с выделением достаточного объёма CPU и памяти для приложения и компонентов мониторинга.
2. Java-приложение собирается в Docker-образ и публикуется в публичном репозитории Docker Hub.
3. В кластере создаётся отдельное пространство имён для приложения, туда разворачиваются Deployment и Service. Приложение поднимается в нескольких экземплярах, Traffic распределяется через сервис.
4. Подключается metrics-server, в Deployment указываются запросы и лимиты по ресурсам. На основе этого конфигурируется HorizontalPodAutoscaler, который отслеживает загрузку CPU и при необходимости увеличивает или уменьшает число реплик.
5. Через Helm устанавливается стек kube-prometheus-stack в отдельный namespace для мониторинга. Он включает Prometheus, Alertmanager, Grafana и вспомогательные компоненты для сбора метрик кластера.
6. Для приложения настраивается сбор метрик в формате Prometheus: Spring Boot экспонирует метрики через endpoint Actuator, а сервис приложения аннотируется так, чтобы Prometheus автоматически начал его скрейпить.
7. В Grafana добавляется источник данных Prometheus и импортируются готовые дашборды для JVM и Spring Boot. На них отображаются метрики нагрузки и состояния сервиса.

## Мониторинг и наблюдаемость

Мониторинг состоит из двух уровней:

- **Уровень кластера** — kube-prometheus-stack собирает метрики с нод и системных компонентов Kubernetes (apiserver, scheduler, kubelet, coredns и др.). Это позволяет контролировать общее состояние кластера.
- **Уровень приложения** — Prometheus собирает метрики с endpoint приложения. На дашбордах Grafana видны:
    - использование памяти JVM;
    - загрузка CPU;
    - частота HTTP-запросов и время ответа;
    - показатели сборщика мусора и другие системные метрики.

Эти дашборды используются для оценки поведения приложения под нагрузкой и проверки работы автоскейлинга.

## Автомасштабирование

Горизонтальное масштабирование реализовано через HPA:

- минимальное и максимальное количество реплик задаётся в манифесте;
- целевой процент загрузки CPU рассчитывается относительно `requests` в Deployment;
- при росте нагрузки HPA увеличивает число подов, при снижении — уменьшает.

В отчёте приведены результаты наблюдений за HPA (из `kubectl describe hpa`) и примеры того, как меняется количество реплик при нагрузке.

## Особенности и ограничения

При работе на Minikube в облачной ВМ выявлены несколько практических моментов:

- после перезапуска виртуальной машины проще пересоздать кластер Minikube и заново применить манифесты, чем пытаться «лечить» потерявший связь control plane;
- при работе с многонодовым Minikube использование встроенного registry-addon оказалось менее удобным, чем публикация образа в Docker Hub;
- установка полноценного стека мониторинга (Prometheus + Grafana) на том же кластере требует достаточного объёма ресурсов и аккуратного перезапуска после изменений.

Все эти нюансы отражены и прокомментированы в отчёте по инфраструктуре.

## Связанные документы

- Подробный пошаговый отчёт по развёртыванию инфраструктуры, включая команды и скриншоты из консоли и Grafana.
- Отдельный отчёт по анализу статей, связанный с тематикой распределения ресурсов, serverless-подходов и RL-планирования задач в контексте данной работы.
